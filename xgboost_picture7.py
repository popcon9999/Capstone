import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error, r2_score
import pickle
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import pvlib
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.dates as mdates
from sklearn.linear_model import LinearRegression

# --- í•œê¸€ í°íŠ¸ ì„¤ì • ---
# ë§‘ì€ ê³ ë”•(Malgun Gothic) í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. (ìœˆë„ìš° ê¸°ì¤€)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€
# ----------------------

def load_and_process_data(power_filepaths, weather_filepaths):
    """
    ì—¬ëŸ¬ ê°œì˜ ë°œì „ëŸ‰ CSVì™€ ë‚ ì”¨ CSVë¥¼ ë¶ˆëŸ¬ì™€ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í†µí•©í•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    print("--- 1. ë°ì´í„° í†µí•© ë° ì „ì²˜ë¦¬ ì‹œì‘ ---")
    
    df_list = []
    
    for filepath in power_filepaths:
        try:
            print(f"\n>>> íŒŒì¼ ì²˜ë¦¬ ì‹œë„: {filepath}")
            df = pd.read_csv(filepath, encoding='cp949', header=0, skipinitialspace=True)
            print(" Â  Â - Pandasë¡œ ë°ì´í„°í”„ë ˆì„ ë¡œë”© ì„±ê³µ!")
            
            df.columns = df.columns.str.strip().str.replace(r'\s+', ' ', regex=True)
            if 'ë°œì „êµ¬ë¶„' in df.columns:
                df['ë°œì „êµ¬ë¶„'] = df['ë°œì „êµ¬ë¶„'].str.strip()
                
                print(f" Â  Â - 'ë°œì „êµ¬ë¶„' ì»¬ëŸ¼ì˜ ê³ ìœ ê°’: {df['ë°œì „êµ¬ë¶„'].unique()}")
                
                df_gyeongsang = df[df['ë°œì „êµ¬ë¶„'] == 'ê²½ìƒëŒ€íƒœì–‘ê´‘'].copy()
                if not df_gyeongsang.empty:
                    print(" Â  Â -> 'ê²½ìƒëŒ€íƒœì–‘ê´‘' ë°ì´í„° ì°¾ìŒ!")
                    df_list.append(df_gyeongsang)
                else:
                    print(" Â  Â -> 'ê²½ìƒëŒ€íƒœì–‘ê´‘' ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨.")
            else:
                print(" Â  Â -> 'ë°œì „êµ¬ë¶„' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f" Â  Â - íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    if not df_list:
        print("\nìµœì¢…ì ìœ¼ë¡œ ì²˜ë¦¬í•  ë°œì „ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    combined_power_df = pd.concat(df_list, ignore_index=True)
    
    id_vars = ['ì¼ì']
    value_vars = [f'{i}ì‹œ ë°œì „ëŸ‰(MWh)' for i in range(1, 25)]
    power_long_df = pd.melt(combined_power_df, id_vars=id_vars, value_vars=value_vars, 
                            var_name='ì‹œê°„', value_name='ë°œì „ëŸ‰(MWh)')

    power_long_df['ì‹œê°„'] = power_long_df['ì‹œê°„'].str.extract('(\d+)').astype(int)
    power_long_df['ì¼ì'] = pd.to_datetime(power_long_df['ì¼ì'], errors='coerce')
    power_long_df.loc[power_long_df['ì‹œê°„'] == 24, 'ì¼ì'] += pd.to_timedelta(1, unit='d')
    power_long_df.loc[power_long_df['ì‹œê°„'] == 24, 'ì‹œê°„'] = 0
    power_long_df['ì¼ì‹œ'] = power_long_df.apply(lambda row: row['ì¼ì'].replace(hour=row['ì‹œê°„']), axis=1)
    
    weather_df_list = []
    for filepath in weather_filepaths:
        try:
            df = pd.read_csv(filepath, encoding='cp949')
            weather_df_list.append(df)
        except Exception as e:
            print(f"ë‚ ì”¨ íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {filepath} - {e}")
            
    if not weather_df_list:
        print("ì²˜ë¦¬í•  ë‚ ì”¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    weather_df = pd.concat(weather_df_list, ignore_index=True)
    
    weather_df.rename(columns={'ì „ìš´ëŸ‰(10ë¶„ìœ„)': 'ìš´ëŸ‰'}, inplace=True)
    weather_df['ì¼ì‹œ'] = pd.to_datetime(weather_df['ì¼ì‹œ'])
    
    # --- ğŸ”§ [í•µì‹¬ ìˆ˜ì •] Merge ì „, ë‘ 'ì¼ì‹œ' ì»¬ëŸ¼ì˜ í˜•ì‹ì„ ë™ì¼í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ---
    # ì´ˆ(second) ì •ë³´ë¥¼ ëª¨ë‘ ì œê±°í•˜ì—¬ 'YYYY-MM-DD HH:MM' í˜•ì‹ìœ¼ë¡œ í†µì¼
    power_long_df['ì¼ì‹œ'] = power_long_df['ì¼ì‹œ'].dt.floor('T')
    weather_df['ì¼ì‹œ'] = weather_df['ì¼ì‹œ'].dt.floor('T')
    # --------------------------------------------------------------------

    # 5. ë°œì „ëŸ‰ ë°ì´í„°ì™€ ë‚ ì”¨ ë°ì´í„° ë³‘í•©
    final_df = pd.merge(power_long_df[['ì¼ì‹œ', 'ë°œì „ëŸ‰(MWh)']], weather_df, on='ì¼ì‹œ', how='inner')
    
    final_df.sort_values(by='ì¼ì‹œ', inplace=True)
    final_df.reset_index(drop=True, inplace=True)

    print(f"--- ë°ì´í„° í†µí•© ì™„ë£Œ. ìµœì¢… ë°ì´í„° ìˆ˜: {len(final_df)}ê°œ ---")
    
    # [ë””ë²„ê¹…] ë³‘í•© í›„ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸
    print("\n--- ë³‘í•© í›„ ë°ì´í„° ìƒ˜í”Œ ---")
    print(final_df.head())
    
    return final_df

def feature_engineering(df):
    """AIê°€ í•™ìŠµí•˜ê¸° ì¢‹ì€ íŠ¹ì§•(Feature)ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("--- 2. íŠ¹ì§• ê³µí•™ ì‹œì‘ ---")
    
    # ê¸°ë³¸ ì „ì²˜ë¦¬
    columns_to_drop = [
        'ì§€ì ', 'ì§€ì ëª…', 'ê¸°ì˜¨ QCí”Œë˜ê·¸', 'ìŠµë„ QCí”Œë˜ê·¸', 
        'ì¼ì¡° QCí”Œë˜ê·¸', 'ì¼ì‚¬ QCí”Œë˜ê·¸', 'ìš´í˜•(ìš´í˜•ì•½ì–´)'
    ]
    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')
    
    # ğŸ”§ [ìˆ˜ì • 1] ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ì‹ ë³€ê²½: 0ìœ¼ë¡œ ì±„ìš°ê¸° ì „ì— ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)

    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì§•
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['season'] = (df['month'] % 12 + 3) // 3 
    
    # --- ğŸ”§ [ê°œì„  1] pvlibë¥¼ ì´ìš©í•œ ë¬¼ë¦¬ ëª¨ë¸ íŠ¹ì§• ì¶”ê°€ ---
    # ì§„ì£¼ì‹œ ìœ„ì¹˜ ì •ë³´ (ìœ„ë„, ê²½ë„, ê³ ë„)
    location = pvlib.location.Location(latitude=35.19, longitude=128.08, altitude=30, tz='Asia/Seoul')
    
    # 'ì¼ì‹œ' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•˜ê³ , ì„œìš¸ ì‹œê°„ëŒ€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¶€ì—¬í•©ë‹ˆë‹¤.
    times = pd.DatetimeIndex(df['ì¼ì‹œ']).tz_localize('Asia/Seoul')
    
    # ì‹œê°„ëŒ€ ì •ë³´ê°€ í¬í•¨ëœ timesë¥¼ ì‚¬ìš©í•˜ì—¬ íƒœì–‘ ìœ„ì¹˜ì™€ ì²­ì²œ ì¼ì‚¬ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    solar_position = location.get_solarposition(times)
    clear_sky = location.get_clearsky(times, model='ineichen')
    
    # ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ì›ë˜ ë°ì´í„°í”„ë ˆì„ì— ë‹¤ì‹œ í• ë‹¹í•©ë‹ˆë‹¤.
    df['solar_azimuth'] = solar_position['azimuth'].values
    df['solar_altitude'] = solar_position['apparent_elevation'].values
    df['clearsky_ghi'] = clear_sky['ghi'].values
    
    # --- âœ¨ [ì—…ê·¸ë ˆì´ë“œ] ì²­ëª…ë„ ì§€ìˆ˜(Cloudiness Index) íŠ¹ì§• ì¶”ê°€ ---
    # 'ì¼ì‚¬(MJ/m2)'ëŠ” ì‹œê°„ë‹¹ ì—ë„ˆì§€, 'clearsky_ghi'ëŠ” ìˆœê°„ íŒŒì›Œ(W/m2)ì´ë¯€ë¡œ ë‹¨ìœ„ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.
    # 1 MJ/hr = 1,000,000 J / 3600 s = 277.7 W
    MJ_to_W_conversion_factor = 1000000 / 3600
    df['ghi_actual_watt'] = df['ì¼ì‚¬(MJ/m2)'] * MJ_to_W_conversion_factor
    
    # ì²­ëª…ë„ ì§€ìˆ˜ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì˜¤ë¥˜ ë°©ì§€)
    # clearsky_ghiê°€ 0ë³´ë‹¤ í´ ë•Œë§Œ ê³„ì‚°í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
    df['cloudiness_index'] = np.where(
        df['clearsky_ghi'] > 0,
        df['ghi_actual_watt'] / df['clearsky_ghi'],
        0
    )
    # ë¶ˆê°€ëŠ¥í•œ ê°’(1 ì´ˆê³¼)ì€ 1ë¡œ ì œí•œí•©ë‹ˆë‹¤. (ì¸¡ì • ì˜¤ì°¨ ë“± ë³´ì •)
    df['cloudiness_index'] = df['cloudiness_index'].clip(0, 1)

    # ê³„ì‚°ì— ì‚¬ìš©ëœ ì¤‘ê°„ ì»¬ëŸ¼ì€ ì‚­ì œ
    df.drop(columns=['ghi_actual_watt'], inplace=True)

    print("ì²­ëª…ë„ ì§€ìˆ˜(cloudiness_index) íŠ¹ì§• ì¶”ê°€ ì™„ë£Œ!")
    
    # feature_engineering í•¨ìˆ˜ì— ì¶”ê°€
    # ì¼ì‚¬ëŸ‰ì˜ 3ì‹œê°„ ì´ë™ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ 'ë³€ë™ì„±' íŠ¹ì§•ìœ¼ë¡œ í™œìš©
    df['irradiance_volatility_3h'] = df['ì¼ì‚¬(MJ/m2)'].rolling(window=3).std()
    
    # ì´ˆë°˜ ê²°ì¸¡ì¹˜ëŠ” ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
    df['irradiance_volatility_3h'].fillna(method='ffill', inplace=True)

    # ì£¼ê¸°ì„± íŠ¹ì§•
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)
    
    print("\n--- ì´ìƒì¹˜ ì œê±° ì‹œì‘ ---")
    
    clearsky_threshold = 200
    power_threshold = 1.0
    
    # â— [ìˆ˜ì •] dfë¥¼ ì‚¬ìš©í•˜ê³ , ì œê±° í›„ ê²°ê³¼ë¥¼ ë‹¤ì‹œ dfì— í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.
    abnormal_indices = df[
        (df['clearsky_ghi'] > clearsky_threshold) &
        (df['ë°œì „ëŸ‰(MWh)'] < power_threshold)
    ].index
    print(f">>> ì œê±° ëŒ€ìƒ ì´ìƒì¹˜ {len(abnormal_indices)}ê°œ ë°œê²¬")
    
    abnormal_indices2 = df[
        (df['clearsky_ghi'] > 600) &      # ë§¤ìš° ë§‘ì€ ë‚  (ì •ì˜¤ ë¶€ê·¼)
        (df['cloudiness_index'] < 0.25)   # ë°œì „ íš¨ìœ¨ì´ 25% ë¯¸ë§Œì¸ ê²½ìš°
    ].index
    print(f">>> ê·œì¹™ 2 (ë‚®ì€ íš¨ìœ¨) ëŒ€ìƒ: {len(abnormal_indices2)}ê°œ")
    
    abnormal_indices3 = abnormal_indices.union(abnormal_indices2)
    print(f">>> ìµœì¢… ì œê±° ëŒ€ìƒ ì´ìƒì¹˜ {len(abnormal_indices3)}ê°œ ë°œê²¬")
    
    
    df = df.drop(index=abnormal_indices3)
    
    print(f">>> ì œê±° í›„ ë°ì´í„° ìˆ˜: {len(df)}ê°œ")
    print("--- ì´ìƒì¹˜ ì œê±° ì™„ë£Œ ---\n")
    

    # ê³¼ê±° ë°ì´í„° íŠ¹ì§• (Lag Features)
    for i in range(1, 8):
        df[f'power_lag_{i * 24}h'] = df['ë°œì „ëŸ‰(MWh)'].shift(i * 24)
    
    for i in range(1, 7): # 1, 2, 3ì‹œê°„ ì „ Lag ì¶”ê°€
        df[f'power_lag_{i}h'] = df['ë°œì „ëŸ‰(MWh)'].shift(i)
    
    def q25(x):
        return x.quantile(0.25)
    def q75(x):
        return x.quantile(0.75)
    
    # ì´ë™í‰ê·  íŠ¹ì§• ì¶”ê°€
    df['temp_roll_mean_3h']= df['ê¸°ì˜¨(Â°C)'].rolling(window=3, min_periods=1).mean()
    df['humidity_roll_mean_3h']=df['ìŠµë„(%)'].rolling(window=3, min_periods=1).mean()
    df['cloud_roll_mean_3h']=df['ìš´ëŸ‰'].rolling(window=3, min_periods=1).mean()
    
    df['power_roll_mean_3h']=df['ë°œì „ëŸ‰(MWh)'].shift(1).rolling(window=3, min_periods=1).mean()

    # ê³„ì ˆ/ì‹œê°„ë³„ í†µê³„ íŠ¹ì§•
    seasonal_hourly_stats = df.groupby(['season', 'hour'])['ë°œì „ëŸ‰(MWh)'].agg(['mean', 'median',q25, q75]).reset_index()
    seasonal_hourly_stats.columns=['season', 'hour', 'seasonal_hourly_mean', 'seasonal_hourly_median', 'seasonal_hourly_q25', 'seasonal_hourly_q75']

    df['season'] = df['season'].astype('int64')
    df['hour'] = df['hour'].astype('int64')
    seasonal_hourly_stats['season'] = seasonal_hourly_stats['season'].astype('int64')
    seasonal_hourly_stats['hour'] = seasonal_hourly_stats['hour'].astype('int64')

    df = pd.merge(df, seasonal_hourly_stats, on=['season', 'hour'], how='left')

    df.dropna(inplace=True)
    print("--- íŠ¹ì§• ê³µí•™ ì™„ë£Œ ---\n")
    print(f"--- ìµœì¢… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜: {len(df)} ---")
    
    return df


# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == '__main__':
    power_files = [
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„7ì›”3ì¼~9).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„7ì›”10~16).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„7ì›”17~23).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„7ì›”24~30).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„7ì›”31ì¼~6ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„8ì›”7~13).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„8ì›”14~20).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„8ì›”21~27).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„8ì›”28~3).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„9ì›”4~10).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„9ì›”11~17).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„9ì›”18~24).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„9ì›”25ì¼~1).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„10ì›”2~8).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„10ì›”9~15).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„10ì›”16~22).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„10ì›”23~29).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„10ì›”30ì¼~5ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„11ì›”6~12).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„11ì›”13~19).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„11ì›”20~26).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„11ì›”27~3).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„12ì›”4ì¼~10).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„12ì›”11~17).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„12ì›”18~24).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (24ë…„12ì›”25~31ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (1ì›”1ì¼~1ì›”6ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (1ì›”7ì¼~1ì›”13ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (1ì›”14ì¼~1ì›”20ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (1ì›”21ì¼~1ì›”27ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (1ì›”28ì¼~2ì›”3ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (2ì›”4ì¼~2ì›”10ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (2ì›”11ì¼~2ì›”17ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (2ì›”18ì¼~2ì›”24ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (2ì›”25ì¼~3ì›”3ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (3ì›”4ì¼~3ì›”10ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (3ì›”11ì¼~3ì›”17ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (3ì›”18ì¼~3ì›”24ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (3ì›”25ì¼~3ì›”31ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (4ì›”1ì¼~4ì›”7ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (4ì›”8ì¼~4ì›”14ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (4ì›”15ì¼~4ì›”21ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (4ì›”22ì¼~4ì›”28ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (4ì›”29ì¼~5ì›”5ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (5ì›”6ì¼~5ì›”12ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (5ì›”13ì¼~5ì›”19ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (5ì›”20ì¼~5ì›”26ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (5ì›”27ì¼~6ì›”2ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (6ì›”3ì¼~6ì›”9ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (6ì›”10ì¼~6ì›”16ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (6ì›”17ì¼~6ì›”23ì¼).csv',
        'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/í•œêµ­ë‚¨ë™ë°œì „_ì‹œê°„ëŒ€ë³„_íƒœì–‘ê´‘_ë°œì „ì‹¤ì (6ì›”24ì¼~6ì›”30ì¼).csv'
    ]
    weather_files = ['c:/Users/ms/Desktop/í•™ìŠµìë£Œ/OBS_ASOS_TIM_20250725134932.csv',
    'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/OBS_ASOS_TIM_20250728101227.csv',
    'c:/Users/ms/Desktop/í•™ìŠµìë£Œ/OBS_ASOS_TIM_20250728101447.csv'
    ]

    initial_data = load_and_process_data(power_files, weather_files)
    if not initial_data.empty:
        featured_data = feature_engineering(initial_data)
        
        timestamps_to_remove_str = [
            '2025-04-26 11:00:00', 
            '2025-05-04 12:00:00',
            '2025-05-29 16:00:00',
            '2025-05-29 17:00:00'
        ]
        timestamps_to_remove = pd.to_datetime(timestamps_to_remove_str)
        
        indices_to_drop_manual = featured_data[
            featured_data['ì¼ì‹œ'].isin(timestamps_to_remove)
        ].index
        
        if not indices_to_drop_manual.empty:
            print(f"\n--- ìˆ˜ë™ ì´ìƒì¹˜ ì œê±° ì‹œì‘ ---")
            print(f">>> ìˆ˜ë™ ì œê±° ëŒ€ìƒ {len(indices_to_drop_manual)}ê°œ ë°œê²¬")
            featured_data = featured_data.drop(index=indices_to_drop_manual)
            print(f">>> ì œê±° í›„ ë°ì´í„° ìˆ˜: {len(featured_data)}ê°œ")
            print("--- ìˆ˜ë™ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ ---\n")
        else:
            print("\n>>> ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•œ ë‚ ì§œê°€ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n")

        features = [
            'ê¸°ì˜¨(Â°C)', 'ìŠµë„(%)', 'ìš´ëŸ‰', 'hour', 'dayofweek',
            'hour_sin', 'hour_cos',
            'power_lag_24h', 'power_lag_48h', 'power_lag_72h', 'power_lag_96h',
            'power_lag_120h', 'power_lag_144h', 'power_lag_168h',
            'seasonal_hourly_mean', 'seasonal_hourly_median',
            'ì¼ì‚¬(MJ/m2)', 'ì¼ì¡°(hr)','seasonal_hourly_q25', 'seasonal_hourly_q75',
            'solar_azimuth', 'solar_altitude', 'clearsky_ghi' , 'power_lag_1h','power_lag_2h','power_lag_3h',
            'power_lag_4h','power_lag_5h','power_lag_6h','temp_roll_mean_3h','humidity_roll_mean_3h','cloud_roll_mean_3h',
            'power_roll_mean_3h', 'irradiance_volatility_3h', 
        ]
        target = 'ë°œì „ëŸ‰(MWh)'
        
        X = featured_data[features]
        y_original = featured_data[target]

        # ğŸ”§ [ìˆ˜ì • 2] íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜ (0 ê°’ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ log1p ì‚¬ìš©)
        y = np.log1p(y_original)

        split_point = int(len(featured_data) * 0.8)
        X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
        y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]
        print(f"--- 3. ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ (ì‹œê³„ì—´ ë°©ì‹). í›ˆë ¨ìš©: {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸ìš©: {len(X_test)}ê°œ ---\n")

        import xgboost as xgb
from sklearn.metrics import mean_absolute_percentage_error, r2_score

# ...

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': 0
    }
    model = xgb.XGBRegressor(**params)
    tscv = TimeSeriesSplit(n_splits=5)
    mape_scores = []
    
    y_train_original = np.expm1(y_train)
    threshold = y_train_original.max() * 0.05
    
    for train_index, val_index in tscv.split(X_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]
        
        model.fit(X_train_fold, y_train_fold)
        
        preds_log = model.predict(X_val_fold)
        preds = np.expm1(preds_log)
        y_val_original = np.expm1(y_val_fold)
        
        meaningful_mask = y_val_original > threshold

        if meaningful_mask.sum() == 0:
            mape_scores.append(1.0)
            continue

        mape = mean_absolute_percentage_error(
            y_val_original[meaningful_mask],
            preds[meaningful_mask]
        )
        mape_scores.append(mape)
    return np.mean(mape_scores)

print("--- 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ (Optuna with XGBoost) ---")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

print("íŠœë‹ ì™„ë£Œ! ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:", study.best_params)
print(f"íŠœë‹ í›„ ìµœì†Œ MAPE (ìˆ˜ì •ëœ ê¸°ì¤€): {study.best_value * 100:.2f}%\n")

print("--- 5. ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ---")
final_model = xgb.XGBRegressor(**study.best_params, random_state=42)
final_model.fit(X_train, y_train)

final_predictions_log = final_model.predict(X_test)
final_predictions = np.expm1(final_predictions_log)

y_test_original = np.expm1(y_test)
y_train_original = np.expm1(y_train)

r2 = r2_score(y_test_original, final_predictions)

threshold = y_train_original.max() * 0.05
meaningful_mask = y_test_original > threshold

final_mape = 0.0
if meaningful_mask.sum() > 0:
    final_mape = mean_absolute_percentage_error(
        y_test_original[meaningful_mask],
        final_predictions[meaningful_mask]
    )

print("-> ìµœì¢… í‰ê°€ ê²°ê³¼:")
print(f"-> R-squared: {r2:.3f}")
print(f"-> ì£¼ê°„ ë°ì´í„° MAPE (ìˆ˜ì •ëœ ê¸°ì¤€): {final_mape * 100:.2f}% (ëª©í‘œ: 15% ì´ë‚´)")

if final_mape > 0 and final_mape <= 0.15:
    with open('solar_final_model_real_data_xgb.pkl', 'wb') as f:
        pickle.dump(final_model, f)
    print("\n--- ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! ìµœì¢… ëª¨ë¸ì„ 'solar_final_model_real_data_xgb.pkl' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ---")
    
    # --- ğŸ”§ [í•µì‹¬ ì¶”ê°€] ì‹¤ì‹œê°„ ì˜ˆì¸¡ì— ì‚¬ìš©í•  í†µê³„ ë°ì´í„° ì €ì¥ ---
    print("\n--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ìš© í†µê³„ ë°ì´í„° ìƒì„± ë° ì €ì¥ ì‹œì‘ ---")
        
        # í›ˆë ¨ ë°ì´í„° ì „ì²´(X_train, y_trainì„ í•©ì¹œ ë°ì´í„°)ë¥¼ ì‚¬ìš©í•˜ì—¬ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    train_df = featured_data.iloc[:split_point]

        # feature_engineering í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ í†µê³„ íŠ¹ì§•ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    def q25(x): return x.quantile(0.25)
    def q75(x): return x.quantile(0.75)
    seasonal_hourly_stats = train_df.groupby(['season', 'hour'])['ë°œì „ëŸ‰(MWh)'].agg(['mean', 'median', q25, q75]).reset_index()
    seasonal_hourly_stats.columns = ['season', 'hour', 'seasonal_hourly_mean', 'seasonal_hourly_median', 'seasonal_hourly_q25', 'seasonal_hourly_q75']
        
    # ê³„ì‚°ëœ í†µê³„ ë°ì´í„°ë¥¼ ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    with open('historical_stats.pkl', 'wb') as f:
        pickle.dump(seasonal_hourly_stats, f)
    print("--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ìš© í†µê³„ ë°ì´í„°('historical_stats.pkl')ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ---")
        # ----------------------------------------------------
    
else:
    print("\n--- ëª©í‘œ ì„±ëŠ¥ì— ì•„ì§ ë„ë‹¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ---")




# --- ğŸ”§ [ê°œì„ ] 6. ìµœì¢… ê²°ê³¼ ì‹œê°í™” ---
print("\n--- 6. ìµœì¢… ê²°ê³¼ ì‹œê°í™” ---")

# ì‹œê°í™”ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ì…‹ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.

test_dates = featured_data.iloc[split_point:]['ì¼ì‹œ'].reset_index(drop=True)

results_df = pd.DataFrame({
'ì¼ì‹œ': test_dates,
'actual': np.expm1(y_test).reset_index(drop=True),
'predicted': final_predictions
})
results_df.set_index('ì¼ì‹œ', inplace=True)

# 1. ì˜ˆì¸¡-ì‹¤ì œ ë¹„êµ ì‚°ì ë„
plt.figure(figsize=(8, 8))
sns.scatterplot(x='actual', y='predicted', data=results_df, alpha=0.5, color='darkgreen', edgecolor=None)
plt.plot([0, results_df['actual'].max()], [0, results_df['actual'].max()], 'r--', label='ì •í™• ì˜ˆì¸¡ì„ ')
plt.title('ì‹¤ì œ ë°œì „ëŸ‰ vs ì˜ˆì¸¡ ë°œì „ëŸ‰ (ì‚°ì ë„)', fontsize=14)
plt.xlabel('ì‹¤ì œ ë°œì „ëŸ‰ (MWh)', fontsize=12)
plt.ylabel('ì˜ˆì¸¡ ë°œì „ëŸ‰ (MWh)', fontsize=12)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. ì‹œê°„ì— ë”°ë¥¸ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê³„ì—´ ê·¸ë˜í”„ (ìƒ˜í”Œë§í•˜ì—¬ ì¼ë¶€ë§Œ í™•ì¸)
sample_interval = 1440  # í•˜ë£¨ ë‹¨ìœ„: 1440ë¶„, ì£¼ ë‹¨ìœ„: 10080ë¶„ ë“± ì¡°ì • ê°€ëŠ¥
for i in range(0, len(results_df), sample_interval):
    sample = results_df.iloc[i:i + sample_interval]
    if len(sample) < 10:  # ë„ˆë¬´ ì ì€ ë°ì´í„°ëŠ” ìŠ¤í‚µ
        continue

    plt.figure(figsize=(18, 5))
    plt.plot(sample.index, sample['actual'], label='ì‹¤ì œ ë°œì „ëŸ‰', color='steelblue', linewidth=2)
    plt.plot(sample.index, sample['predicted'], label='ì˜ˆì¸¡ ë°œì „ëŸ‰', color='orangered', linestyle='--', linewidth=2)
    plt.title(f'ì£¼ê°„ ë°œì „ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ {i} ~ {i + sample_interval})', fontsize=14)
    plt.xlabel('ì‹œê°„', fontsize=12)
    plt.ylabel('ë°œì „ëŸ‰ (MWh)', fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()



    # ì‹œê°„ í¬ë§·ì„ ìë™ ì¡°ì •
    ax = plt.gca()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
    plt.show()
    
    
from sklearn.linear_model import LinearRegression
import numpy as np

plt.figure(figsize=(8, 8))
sns.scatterplot(x='actual', y='predicted', data=results_df, alpha=0.5, color='darkgreen', edgecolor=None)

# ì •í™• ì˜ˆì¸¡ì„ 
max_val = max(results_df['actual'].max(), results_df['predicted'].max())
plt.plot([0, max_val], [0, max_val], 'r--', label='ì •í™• ì˜ˆì¸¡ì„ ')

# ì¶”ì„¸ì„ 
lr = LinearRegression()
lr.fit(results_df[['actual']], results_df['predicted'])
x_vals = np.linspace(0, max_val, 100)
y_vals = lr.predict(x_vals.reshape(-1, 1))
plt.plot(x_vals, y_vals, 'b-', label='ì¶”ì„¸ì„ ')

plt.title('ì‹¤ì œ ë°œì „ëŸ‰ vs ì˜ˆì¸¡ ë°œì „ëŸ‰', fontsize=14)
plt.xlabel('ì‹¤ì œ ë°œì „ëŸ‰ (MWh)')
plt.ylabel('ì˜ˆì¸¡ ë°œì „ëŸ‰ (MWh)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(18, 6))
plt.plot(results_df.index, results_df['actual'], label='ì‹¤ì œ ë°œì „ëŸ‰', color='steelblue', linewidth=1)
plt.plot(results_df.index, results_df['predicted'], label='ì˜ˆì¸¡ ë°œì „ëŸ‰', color='orangered', linestyle='--', linewidth=1)
plt.title('ì‹œê°„ì— ë”°ë¥¸ ì „ì²´ ë°œì „ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼', fontsize=14)
plt.xlabel('ì‹œê°„')
plt.ylabel('ë°œì „ëŸ‰ (MWh)')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)

# ì‹œê°„ í¬ë§· ìë™ ì¡°ì •
ax = plt.gca()
ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
plt.tight_layout()
plt.show()


results_df['hour'] = results_df.index.hour
hourly_avg = results_df.groupby('hour')[['actual', 'predicted']].mean()

plt.figure(figsize=(10, 5))
hourly_avg.plot(kind='bar', ax=plt.gca(), color=['skyblue', 'salmon'])
plt.title('ì‹œê°„ëŒ€ë³„ í‰ê·  ë°œì „ëŸ‰ ë¹„êµ')
plt.xlabel('ì‹œê°„ (Hour of Day)')
plt.ylabel('í‰ê·  ë°œì „ëŸ‰ (MWh)')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.legend(['ì‹¤ì œ ë°œì „ëŸ‰', 'ì˜ˆì¸¡ ë°œì „ëŸ‰'])
plt.show()

results_df['residual'] = results_df['actual'] - results_df['predicted']

plt.figure(figsize=(18, 5))
plt.plot(results_df.index, results_df['residual'], color='purple', linewidth=0.8)
plt.title('ì‹œê°„ì— ë”°ë¥¸ ì˜ˆì¸¡ ì˜¤ì°¨(Residual)', fontsize=14)
plt.xlabel('ì‹œê°„')
plt.ylabel('ì˜¤ì°¨ (MWh)')
plt.grid(True)
plt.tight_layout()

ax = plt.gca()
ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
plt.show()

results_df['weekday'] = results_df.index.dayofweek  # 0: ì›”, ..., 6: ì¼
results_df['hour'] = results_df.index.hour

pivot_table = results_df.pivot_table(index='hour', columns='weekday', values='actual', aggfunc='mean')

plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, cmap='YlGnBu', annot=True, fmt=".1f")
plt.title('ì‹œê°„ëŒ€ë³„ ìš”ì¼ë³„ í‰ê·  ì‹¤ì œ ë°œì „ëŸ‰ (MWh)')
plt.xlabel('ìš”ì¼ (0=ì›” ~ 6=ì¼)')
plt.ylabel('ì‹œê°„ (Hour)')
plt.tight_layout()
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(results_df['actual'], results_df['predicted'])
mape = mean_absolute_percentage_error(results_df['actual'], results_df['predicted']) * 100
rmse = np.sqrt(mean_squared_error(results_df['actual'], results_df['predicted']))

metrics = pd.DataFrame({
    'ì§€í‘œ': ['MAE', 'MAPE', 'RMSE'],
    'ê°’': [mae, mape, rmse]
})

plt.figure(figsize=(6, 4))
sns.barplot(x='ì§€í‘œ', y='ê°’', data=metrics, palette='Set2')
plt.title('ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ')
plt.ylabel('ê°’')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import matplotlib.dates as mdates

# ê²°ê³¼ DataFrameì˜ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
results_df['month'] = results_df.index.to_period('M')

# ì›”ë³„ë¡œ ë£¨í”„ë¥¼ ëŒë©° ì‹œê³„ì—´ ê·¸ë˜í”„ ì¶œë ¥
for month, group in results_df.groupby('month'):
    if len(group) < 10:  # ë„ˆë¬´ ì ì€ ë°ì´í„°ëŠ” ê±´ë„ˆëœ€
        continue

    plt.figure(figsize=(18, 5))
    plt.plot(group.index, group['actual'], label='ì‹¤ì œ ë°œì „ëŸ‰', color='steelblue', linewidth=1.5)
    plt.plot(group.index, group['predicted'], label='ì˜ˆì¸¡ ë°œì „ëŸ‰', color='orangered', linestyle='--', linewidth=1.5)

    plt.title(f'{month} ë°œì „ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼', fontsize=14)
    plt.xlabel('ì‹œê°„')
    plt.ylabel('ë°œì „ëŸ‰ (MWh)')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    ax = plt.gca()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.tight_layout()
    plt.show()


# 1. ìµœì¢… ëª¨ë¸ì„ ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµ
final_model = xgb.XGBRegressor(**study.best_params, random_state=42)
final_model.fit(X_train, y_train)

# 2. í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
predictions_log = final_model.predict(X_test)

# 3. ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
predictions = np.expm1(predictions_log)
y_test_original = np.expm1(y_test)

# 4. ë¶„ì„ì„ ìœ„í•œ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
# X_testì— ìˆëŠ” ëª¨ë“  íŠ¹ì§•ê³¼ ì‹¤ì œê°’, ì˜ˆì¸¡ê°’ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
error_analysis_df = X_test.copy()
error_analysis_df['Timestamp'] = featured_data.loc[X_test.index, 'ì¼ì‹œ'] # ì‹œê°„ ì •ë³´ ì¶”ê°€
error_analysis_df['Actual_Power'] = y_test_original
error_analysis_df['Predicted_Power'] = predictions
error_analysis_df['Error'] = np.abs(error_analysis_df['Actual_Power'] - error_analysis_df['Predicted_Power'])

print("ì—ëŸ¬ ë¶„ì„ìš© ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ!")
error_analysis_df.head()

# ì—ëŸ¬ê°€ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 50ê°œ í™•ì¸
worst_predictions_df = error_analysis_df.sort_values(by='Error', ascending=False).head(50)

print("ì˜¤ì°¨ê°€ ê°€ì¥ í° ìƒìœ„ 50ê°œ ë°ì´í„°:")
print(worst_predictions_df)